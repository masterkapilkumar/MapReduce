17/10/21 18:30:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/10/21 18:30:27 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
17/10/21 18:30:27 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
17/10/21 18:30:27 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
17/10/21 18:30:27 INFO input.FileInputFormat: Total input files to process : 2
17/10/21 18:30:27 INFO mapreduce.JobSubmitter: number of splits:2
17/10/21 18:30:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local152460488_0001
17/10/21 18:30:28 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
17/10/21 18:30:28 INFO mapreduce.Job: Running job: job_local152460488_0001
17/10/21 18:30:28 INFO mapred.LocalJobRunner: OutputCommitter set in config null
17/10/21 18:30:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/10/21 18:30:28 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17/10/21 18:30:28 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17/10/21 18:30:28 INFO mapred.LocalJobRunner: Waiting for map tasks
17/10/21 18:30:28 INFO mapred.LocalJobRunner: Starting task: attempt_local152460488_0001_m_000000_0
17/10/21 18:30:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/10/21 18:30:28 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17/10/21 18:30:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/10/21 18:30:28 INFO mapred.MapTask: Processing split: hdfs://hadoop-master:9000/wordcount/input/inp2.txt:0+94281021
17/10/21 18:30:28 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/10/21 18:30:28 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/10/21 18:30:28 INFO mapred.MapTask: soft limit at 83886080
17/10/21 18:30:28 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/10/21 18:30:28 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/10/21 18:30:28 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/10/21 18:30:29 INFO mapreduce.Job: Job job_local152460488_0001 running in uber mode : false
17/10/21 18:30:29 INFO mapreduce.Job:  map 0% reduce 0%
17/10/21 18:30:30 INFO mapred.MapTask: Spilling map output
17/10/21 18:30:30 INFO mapred.MapTask: bufstart = 0; bufend = 31674456; bufvoid = 104857600
17/10/21 18:30:30 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 13161496(52645984); length = 13052901/6553600
17/10/21 18:30:30 INFO mapred.MapTask: (EQUATOR) 42160212 kvi 10540048(42160192)
17/10/21 18:30:35 INFO mapred.MapTask: Finished spill 0
17/10/21 18:30:35 INFO mapred.MapTask: (RESET) equator 42160212 kv 10540048(42160192) kvi 7918620(31674480)
17/10/21 18:30:36 INFO mapred.MapTask: Spilling map output
17/10/21 18:30:36 INFO mapred.MapTask: bufstart = 42160212; bufend = 73834002; bufvoid = 104857600
17/10/21 18:30:36 INFO mapred.MapTask: kvstart = 10540048(42160192); kvend = 23701380(94805520); length = 13053069/6553600
17/10/21 18:30:36 INFO mapred.MapTask: (EQUATOR) 84319753 kvi 21079932(84319728)
17/10/21 18:30:40 INFO mapred.LocalJobRunner: map > map
17/10/21 18:30:41 INFO mapred.MapTask: Finished spill 1
17/10/21 18:30:41 INFO mapred.MapTask: (RESET) equator 84319753 kv 21079932(84319728) kvi 18458508(73834032)
17/10/21 18:30:41 INFO mapreduce.Job:  map 15% reduce 0%
17/10/21 18:30:42 INFO mapred.MapTask: Spilling map output
17/10/21 18:30:42 INFO mapred.MapTask: bufstart = 84319753; bufend = 11133328; bufvoid = 104857596
17/10/21 18:30:42 INFO mapred.MapTask: kvstart = 21079932(84319728); kvend = 8026212(32104848); length = 13053721/6553600
17/10/21 18:30:42 INFO mapred.MapTask: (EQUATOR) 21619080 kvi 5404764(21619056)
17/10/21 18:30:46 INFO mapred.LocalJobRunner: map > map
17/10/21 18:30:46 INFO mapred.MapTask: Finished spill 2
17/10/21 18:30:46 INFO mapred.MapTask: (RESET) equator 21619080 kv 5404764(21619056) kvi 2783336(11133344)
17/10/21 18:30:47 INFO mapreduce.Job:  map 21% reduce 0%
17/10/21 18:30:47 INFO mapred.MapTask: Spilling map output
17/10/21 18:30:47 INFO mapred.MapTask: bufstart = 21619080; bufend = 53290437; bufvoid = 104857600
17/10/21 18:30:47 INFO mapred.MapTask: kvstart = 5404764(21619056); kvend = 18565488(74261952); length = 13053677/6553600
17/10/21 18:30:47 INFO mapred.MapTask: (EQUATOR) 63776186 kvi 15944040(63776160)
17/10/21 18:30:52 INFO mapred.MapTask: Finished spill 3
17/10/21 18:30:52 INFO mapred.MapTask: (RESET) equator 63776186 kv 15944040(63776160) kvi 13322616(53290464)
17/10/21 18:30:52 INFO mapred.LocalJobRunner: map > map
17/10/21 18:30:52 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2931)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:688)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:647)
        at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1624)
        at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:892)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:927)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:974)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)
        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:193)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
        at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
        at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:270)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:473)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
        at java.lang.Thread.run(Thread.java:748)
17/10/21 18:30:52 WARN hdfs.DFSClient: Failed to connect to /10.17.6.73:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2931)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:688)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:647)
        at org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1624)
        at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:892)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:927)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:974)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)
        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:193)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
        at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
        at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:270)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:473)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
        at java.lang.Thread.run(Thread.java:748)
17/10/21 18:30:52 INFO hdfs.DFSClient: Successfully connected to /10.17.6.18:50010 for BP-1035421894-127.0.1.1-1504946808916:blk_1073741845_1021
17/10/21 18:30:53 INFO mapreduce.Job:  map 29% reduce 0%
17/10/21 18:30:53 INFO mapred.MapTask: Spilling map output
17/10/21 18:30:53 INFO mapred.MapTask: bufstart = 63776186; bufend = 95447957; bufvoid = 104857600
17/10/21 18:30:53 INFO mapred.MapTask: kvstart = 15944040(63776160); kvend = 2890468(11561872); length = 13053573/6553600
17/10/21 18:30:53 INFO mapred.MapTask: (EQUATOR) 1076106 kvi 269020(1076080)
17/10/21 18:30:53 INFO mapred.LocalJobRunner: map > map
17/10/21 18:30:53 INFO mapred.MapTask: Starting flush of map output
17/10/21 18:30:57 INFO mapred.MapTask: Finished spill 4
17/10/21 18:30:57 INFO mapred.MapTask: (RESET) equator 1076106 kv 269020(1076080) kvi 25807576(103230304)
17/10/21 18:30:57 INFO mapred.MapTask: Spilling map output
17/10/21 18:30:57 INFO mapred.MapTask: bufstart = 1076106; bufend = 2680561; bufvoid = 104857600
17/10/21 18:30:57 INFO mapred.MapTask: kvstart = 269020(1076080); kvend = 25807580(103230320); length = 675841/6553600
17/10/21 18:30:57 INFO mapred.MapTask: Finished spill 5
17/10/21 18:30:57 INFO mapred.Merger: Merging 6 sorted segments
17/10/21 18:30:57 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 192938387 bytes
17/10/21 18:30:58 INFO mapred.LocalJobRunner: map > sort >
17/10/21 18:30:59 INFO mapreduce.Job:  map 34% reduce 0%
17/10/21 18:31:04 INFO mapred.LocalJobRunner: map > sort >
17/10/21 18:31:05 INFO mapreduce.Job:  map 43% reduce 0%
17/10/21 18:31:10 INFO mapred.LocalJobRunner: map > sort >
17/10/21 18:31:10 INFO mapred.Task: Task:attempt_local152460488_0001_m_000000_0 is done. And is in the process of committing
17/10/21 18:31:10 INFO mapred.LocalJobRunner: map > sort
17/10/21 18:31:10 INFO mapred.Task: Task 'attempt_local152460488_0001_m_000000_0' done.
17/10/21 18:31:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local152460488_0001_m_000000_0
17/10/21 18:31:10 INFO mapred.LocalJobRunner: Starting task: attempt_local152460488_0001_m_000001_0
17/10/21 18:31:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/10/21 18:31:10 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17/10/21 18:31:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/10/21 18:31:10 INFO mapred.MapTask: Processing split: hdfs://hadoop-master:9000/wordcount/input/inp.txt:0+4120378
17/10/21 18:31:10 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
17/10/21 18:31:10 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
17/10/21 18:31:10 INFO mapred.MapTask: soft limit at 83886080
17/10/21 18:31:10 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
17/10/21 18:31:10 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
17/10/21 18:31:10 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
17/10/21 18:31:11 INFO mapreduce.Job:  map 100% reduce 0%
17/10/21 18:32:10 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.17.6.73:50010]
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2931)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:688)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:647)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:918)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:974)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)
        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:151)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:191)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
        at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
        at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:270)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:473)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
        at java.lang.Thread.run(Thread.java:748)
17/10/21 18:32:10 WARN hdfs.DFSClient: Failed to connect to /10.17.6.73:50010 for block, add to deadNodes and continue. org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.17.6.73:50010]
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.17.6.73:50010]
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2931)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)
        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:688)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:647)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:918)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:974)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
        at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)
        at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)
        at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:151)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:191)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
        at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
        at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:270)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:473)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
        at java.lang.Thread.run(Thread.java:748)
17/10/21 18:32:10 INFO hdfs.DFSClient: Successfully connected to /10.17.6.18:50010 for BP-1035421894-127.0.1.1-1504946808916:blk_1073741843_1019
17/10/21 18:32:11 INFO mapred.LocalJobRunner:
17/10/21 18:32:11 INFO mapred.MapTask: Starting flush of map output
17/10/21 18:32:11 INFO mapred.MapTask: Spilling map output
17/10/21 18:32:11 INFO mapred.MapTask: bufstart = 0; bufend = 6857843; bufvoid = 104857600
17/10/21 18:32:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23446508(93786032); length = 2767889/6553600
17/10/21 18:32:11 INFO mapreduce.Job:  map 50% reduce 0%
17/10/21 18:32:12 INFO mapred.MapTask: Finished spill 0
17/10/21 18:32:12 INFO mapred.Task: Task:attempt_local152460488_0001_m_000001_0 is done. And is in the process of committing
17/10/21 18:32:12 INFO mapred.LocalJobRunner: map
17/10/21 18:32:12 INFO mapred.Task: Task 'attempt_local152460488_0001_m_000001_0' done.
17/10/21 18:32:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local152460488_0001_m_000001_0
17/10/21 18:32:12 INFO mapred.LocalJobRunner: map task executor complete.
17/10/21 18:32:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks
17/10/21 18:32:12 INFO mapred.LocalJobRunner: Starting task: attempt_local152460488_0001_r_000000_0
17/10/21 18:32:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/10/21 18:32:12 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
17/10/21 18:32:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
17/10/21 18:32:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@17620808
17/10/21 18:32:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10
17/10/21 18:32:12 INFO reduce.EventFetcher: attempt_local152460488_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
17/10/21 18:32:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local152460488_0001_m_000001_0 decomp: 8241791 len: 8241795 to MEMORY
17/10/21 18:32:12 INFO reduce.InMemoryMapOutput: Read 8241791 bytes from map-output for attempt_local152460488_0001_m_000001_0
17/10/21 18:32:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8241791, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8241791
17/10/21 18:32:12 INFO reduce.MergeManagerImpl: attempt_local152460488_0001_m_000000_0: Shuffling to disk since 192938402 is greater than maxSingleShuffleLimit (90821424)
17/10/21 18:32:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local152460488_0001_m_000000_0 decomp: 192938402 len: 192938406 to DISK
17/10/21 18:32:12 INFO mapreduce.Job:  map 100% reduce 0%
17/10/21 18:32:12 INFO reduce.OnDiskMapOutput: Read 192938406 bytes from map-output for attempt_local152460488_0001_m_000000_0
17/10/21 18:32:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
17/10/21 18:32:12 INFO mapred.LocalJobRunner: 2 / 2 copied.
17/10/21 18:32:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 1 on-disk map-outputs
17/10/21 18:32:12 INFO mapred.Merger: Merging 1 sorted segments
17/10/21 18:32:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8241787 bytes
17/10/21 18:32:13 INFO reduce.MergeManagerImpl: Merged 1 segments, 8241791 bytes to disk to satisfy reduce memory limit
17/10/21 18:32:13 INFO reduce.MergeManagerImpl: Merging 2 files, 201180201 bytes from disk
17/10/21 18:32:13 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
17/10/21 18:32:13 INFO mapred.Merger: Merging 2 sorted segments
17/10/21 18:32:13 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 201180180 bytes
17/10/21 18:32:13 INFO mapred.LocalJobRunner: 2 / 2 copied.
17/10/21 18:32:13 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
17/10/21 18:32:14 INFO hdfs.DataStreamer: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
        at org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:226)
        at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1591)
        at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1547)
        at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:658)
17/10/21 18:32:14 WARN hdfs.DataStreamer: Abandoning BP-1035421894-127.0.1.1-1504946808916:blk_1073741847_1023
17/10/21 18:32:14 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[10.17.6.73:50010,DS-44dd804c-f945-42a5-b0a3-09ae524cee0f,DISK]
17/10/21 18:32:23 INFO mapred.Task: Task:attempt_local152460488_0001_r_000000_0 is done. And is in the process of committing
17/10/21 18:32:23 INFO mapred.LocalJobRunner: 2 / 2 copied.
17/10/21 18:32:23 INFO mapred.Task: Task attempt_local152460488_0001_r_000000_0 is allowed to commit now
17/10/21 18:32:23 INFO output.FileOutputCommitter: Saved output of task 'attempt_local152460488_0001_r_000000_0' to hdfs://hadoop-master:9000/wordcount/output/_temporary/0/task_local152460488_0001_r_000000
17/10/21 18:32:23 INFO mapred.LocalJobRunner: reduce > reduce
17/10/21 18:32:23 INFO mapred.Task: Task 'attempt_local152460488_0001_r_000000_0' done.
17/10/21 18:32:23 INFO mapred.LocalJobRunner: Finishing task: attempt_local152460488_0001_r_000000_0
17/10/21 18:32:23 INFO mapred.LocalJobRunner: reduce task executor complete.
17/10/21 18:32:23 INFO mapreduce.Job:  map 100% reduce 100%
17/10/21 18:32:23 INFO mapreduce.Job: Job job_local152460488_0001 completed successfully
17/10/21 18:32:23 INFO mapreduce.Job: Counters: 35
        File System Counters
                FILE: Number of bytes read=981186392
                FILE: Number of bytes written=1376283535
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=291083819
                HDFS: Number of bytes written=908377
                HDFS: Number of read operations=22
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=5
        Map-Reduce Framework
                Map input records=1709351
                Map output records=17177673
                Map output bytes=166824843
                Map output materialized bytes=201180201
                Input split bytes=229
                Combine input records=0
                Combine output records=0
                Reduce input groups=73222
                Reduce shuffle bytes=201180201
                Reduce input records=17177673
                Reduce output records=73222
                Spilled Records=50841046
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=606
                Total committed heap usage (bytes)=456536064
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=98401399
        File Output Format Counters
                Bytes Written=908377